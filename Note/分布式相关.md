[TOC]

# 负载均衡算法

- 轮询：按请求的顺序分配给各个服务器，适用于各台服务器性能相同
- 加权轮询：给各个服务器附上权重值，按权重的高低分配请求，适用于各台服务器性能不同，性能高的服务器权重也高
- 最少链接：将请求发送给当前最少连接数的服务器上
- 加权最少链接：在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数
- IP地址哈希：哈希均匀分布



# 分布式事务

## 理论

### 分布式系统的三个指标

CAP定理

## 2PC

需要有协调者和参与者，协调者负责调度，参与者负责执行，分两步完成，1：prepare阶段 2：commit阶段

在执行阶段，节点是处于阻塞状态，直到commit阶段完成，本地事务才会释放资源，因此性能不佳

* 正常情况下

**prepare阶段**：协调者向参与者A、B发送请求执行操作，参与者A、B开启事务，执行操作，但不commit，操作完成后，告诉协调者已经完成

**commit阶段：**协调者收到参与者的完成响应，向参与者A、B发送commit请求，参与者A、B收到commit请求后，提交事务，完成操作；如果收到执行失败的响应，则发送回滚请求给参与者A、B，执行回滚

* 异常情况下

在协调者等待参与者的完成响应时，协调者或参与者可能宕机，最终会导致数据不一致或阻塞，例如

当处于prepare阶段和commit阶段之间时，协调者挂掉(或挂掉后重启)，会导致协调者收不到参与者的响应，此时协调者就不清楚接下来的commit要发送什么请求过去，或者就不发请求过去了，导致参与者一直阻塞

**解决办法：**协调者维护一份事务日志，以方便宕机重启后恢复原来的状态，但无法为参与者设置超时自动操作，因为它并不知道commit阶段自己要进行commit还是回滚

当处于prepare阶段和commit阶段之间时，参与者挂掉后，接收不到协调者的请求，不知道接下来要执行commit还是回滚，协调者也无法在参与者挂掉后进行回滚操作

**解决办法：**3pc

## 3PC

3pc实际上就是将commit阶段拆成两步，preCommit相当于一次保险阶段，作用类似于2pc的二阶段，但是它不是正真的提交

* 正常情况下

**canCommit阶段：**协调者向参与者发送请求，参与者开启事务执行操作，成功完成后响应Yes，否则响应No

**preCommit阶段：**协调者收到所有参与者的Yes响应，发送操作请求给所有参与者，告诉所有参与者进行预提交状态

**commit阶段：**协调者收到所有参与者的应答响应，向所有参与者发送commit请求，参与者收到后提交事务

* 异常情况下

如果在preCommit阶段到commit阶段之间，协调者挂了，参与者会在超时后进行事务提交

## TCC

补偿事务，每一个操作都要有对应的确认和补偿，类似于2pc，但2pc在于DB层面，TCC在于业务层面，每个业务逻辑都需要实现try-confirm-cancel的操作

**Try阶段**：对于操作的数据行，增加字段表示其状态，表示正在操作

**confirm阶段**：将try阶段中表示数据状态的字段修改为确认状态，表示已经完成操作，操作需要幂等

**cancel阶段**：将try阶段进行的操作进行回滚

通过不断重试，并发的时候还是需要分布式锁

## SAGA



## 利用RocketMQ事务

事务性消息：本地事务和发送消息是原子性操作

https://www.jianshu.com/p/53324ea2df92

http://blog.itpub.net/31556438/viewspace-2649246/



## 阿里云分布式事务GTS






## 参考：

[2pc、3pc](https://zhuanlan.zhihu.com/p/21994882)

[TCC](https://juejin.im/post/5bf201f7f265da610f63528a)

[TCC](https://yemablog.com/posts/tcc-1)

[华为的servicecomb](https://blog.csdn.net/weixin_42075590/article/details/89236625)

[蚂蚁金服的seata分布式事务架构](https://www.sofastack.tech/blog/sofa-meetup-3-seata-retrospect/)

# 分布式理论

## CAP理论

CAP特性

* C：Consistency，一致性，数据状态转化一致，写操作完成后的读操作，可以获取到最新的值
* A：Availability，可用性，指的是服务一直可用，可以正常响应
* P：Partition tolerance，分区容错，指的是当有节点故障不连通时，就会分区，但仍然能对外提供服务

矛盾在于这三个特性不能同时满足，比如

> 当分布式集群内有两个主从服务发生网络故障，但此时服务仍然可以访问，此时具有分区容错性。
>
> 当对主服务对数据进行修改时，由于网络问题，无法同步到从服务，当访问到从服务时，无法获取到最新的值，此时满足可用性，但是无法满足一致性。
>
> 当主从服务间网络恢复，写操作的数据虽然能在服务间同步了，但还未同步完成，此时访问从服务无法获取最新值，此时满足了一致性，但是无法满足可用性。
>
> 简单概括，只要满足分区容错，就会设置复制集，复制集同时也保证了可用，但是复制集又会有数据同步，此时又有一致性问题

所以，一般只会满足其中两个

> 1、满足CA舍弃P，也就是满足一致性和可用性，舍弃容错性。但是这也就意味着你的系统不是分布式的了，因为涉及分布式的想法就是把功能分开，部署到不同的机器上。
>
> 2、满足CP舍弃A，也就是满足一致性和容错性，舍弃可用性。如果你的系统允许有段时间的访问失效等问题，这个是可以满足的。就好比多个人并发买票，后台网络出现故障，你买的时候系统就崩溃了。
>
> 3、满足AP舍弃C，也就是满足可用性和容错性，舍弃一致性。这也就是意味着你的系统在并发访问的时候可能会出现数据不一致的情况。

所以为了分布式服务能正常使用，一般时会满足分区容错性和可用性，在一致性上不追求强一致性，而是一个逐渐一致的过程。

## BASE理论

BASE理论是对CAP三者均衡的结果，基于CAP理论演化而来，通过牺牲强一致性来获得高可用。

* Basically Available（基本可用）: 允许暂时不可用，比如访问时可以等待返回，服务降级，保证核心可用等。
* Soft state（软状态）: 允许系统存在中间状态，而该中间状态不会影响系统整体可用性，比如允许复制集副本间的数据存在延时，数据库的数据同步过程。
* Eventually consistent（最终一致性）: 系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

与数据库ACID类似，只是强度减弱了

参考：[CAP 定理的含义](http://www.ruanyifeng.com/blog/2018/07/cap.html)

# ZooKeeper

ZooKeeper保证的是CP，不保证每次服务请求的可用性，在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。另外在进行leader选举时集群都是不可用，所以说，ZooKeeper不能保证服务可用性。

## 使用场景

* 集群管理，监控节点存活状态
* 主节点选举，当服务以master-salve模式进行部署，当主节点挂掉后选出新的主节点
* 服务发现
* 分布式锁，提供独占锁、共享锁
* 分布式自增id
* 搭配Kafka、dubbo等使用

## 特点

* 顺序一致性：同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。

* 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。

* 单一系统映像：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。

* 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。

## ZAB协议

通过ZAB协议保证注册到ZooKeeper上的主从节点状态同步，该协议有两种模式

* 崩溃恢复

  当整个 Zookeeper 集群刚刚启动或者Leader服务器宕机、重启或者网络故障导致**不存在过半的服务器与 Leader 服务器保持正常通信时，所有服务器进入崩溃恢复模式**，首先选举产生新的 Leader 服务器，然后集群中 Follower 服务器开始与新的 Leader 服务器进行数据同步。

* 消息广播

  当集群中超过半数机器与该 Leader 服务器完成数据同步之后，退出恢复模式进入消息广播模式，Leader 服务器开始接收客户端的事务请求生成事物提案（超过半数同意）来进行事务请求处理。

### 选举算法和流程

ZooKeeper集群机器要求至少三台机器，机器的角色分为Leader、Follower、Observer

默认使用FastLeaderElection算法，比如现在有5台服务器，每台服务器均没有数据，它们的编号分别是1, 2, 3, 4, 5按编号依次启动，它们的选择举过程如下：

1. 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking。
2. 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是Looking。
3. 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为leader，服务器1,2成为Follower。
4. 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为Follower。
5. 服务器5启动，后面的逻辑同服务器4成为Follower。

当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。

1. Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准Leader。
2. Discovery（发现阶段）：在这个阶段，Followers 跟准 Leader 进行通信，同步 followers 最近接收的事务提议。
3. Synchronization（同步阶段）:同步阶段主要是利用 Leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 Leader。
4. Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。

## 通知机制

客户端会对某个znode建立一个watcher事件，当该znode发生变化时，这些客户端会收到ZooKeeper的通知，然后客户端根据znode的变化来做出相应的改变

# 分布式锁



# 分布式自增id

这两部分笔记在本地，待整理



# 分布一致性算法

## Raft算法

参考：[Raft算法详解](https://zhuanlan.zhihu.com/p/32052223)



