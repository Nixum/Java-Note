[TOC]


# 基础架构

## MySQL逻辑架构图

![](G:\git repository\Java-Note\Note\picture\MySQL逻辑结构图.jpg)

* 连接器：负责跟客户端建立连接、获取权限、维持和管理连接。登录进去后修改权限，默认是将在下一次登录后生效
* 查询缓存：MySQL接收到查询请求后会先查询缓存，key是查询语句，value是查询结果，之后经过执行器的权限判断再返回，如果查不到则往后走。不建议使用，因为若有更新操作，会删除对应表的缓存，可能导致缓存命中低，可以设置query_cache_type=demand，默认不使用缓存，需要在查询时显示指定。MySQL8.0删除此功能
* 分析器：对SQL语句进行分析，词法分析判断各个字符串代表的含义（包括列是否存在），语法分析判断SQL的语法是否正确，这一层操作之后，MySQL就知道你要做什么了
* 优化器：决定是否要使用索引，使用哪个索引，决定表的连接顺序
* 执行器：先判断是否有对该表的操作权限，之后判断要使用哪个引擎提供的接口
* 引擎：对数据进行具体操作的执行者，事务和索引都是在这层做的，但具体需要引擎支持，例如MyISAM不支持事务，InnoDB支持

## 日志系统

* redo log重做日志：InnoDB独有，物理日志，记录这个页做了什么改动，使用二阶段提交保证两份日志逻辑一致。记录操作到redo log后状态是prepare，binlog写入磁盘，事务提交，redo log改为commit状态，在写的时候是先写进redo log buffer，commit后才写进redo log(磁盘)

  当有记录要更新的时候，InnoDB会先把记录(包含数据变更和change buffer的变更)写到redo log里，并更新内存，再在恰当的时候更到磁盘里，redo log prepare、commit 的XID对应bin log的XID实现关联。

  InnoDB的redo log是固定大小的，比如有一组4个文件组成的“环形队列”，首位指针表示当前记录的位置和当前擦除位置，擦除前会把记录更新到磁盘，这种能力也称为crash-safe

  建议设置innodb_flush_log_at_trx_commit=1，表示每次事务的redo log会持久化到磁盘

* bin log归档日志：属于server层的日志，逻辑日志，记录所有逻辑操作，追加写入，不会覆盖以前的日志，bin log有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，一般使用row，记录行变化前和变化后的数据，缺点是日志变大。从库是使用bin log进行备份的

  建议设置sync_binlog=1，表示每次事务的bin log都会持久化到磁盘

可以只使用redo log来实现崩溃恢复，但无法只使用bin log，原因是 InnoDB使用WAL机制，执行事务时，写完内存和日志，事务就完成了，如果此时数据库崩溃，要依赖日志来恢复数据页，但是bin log并没有记录数据页的更新细节，而redo log因为环形写入的问题，无法对所有记录进行归档，仅仅只能实现崩溃恢复

备份时间的长短会影响日志文件的大小，文件的完整性，从而影响到恢复速度和恢复效果

# 常用SQL

## Count(\*)、Count(1)、Count([列])区别

在count(*)不带条件在MyISAM里查询比较快，因为MyISAM会存储总条数，不带条件查询的时候直接用就行，而InnoDB带了事务，支持MVCC，因此每次count(\*)时都会扫表

以下归纳基于InnoDB，count会返回满足条件的结果集的总行数，它会使用存储引擎进行全表扫描获取结果，比如count(1)会直接返回1，count(主键)会获取主键，返回给server层，由server层进行计数，因此按效率排序是：count( 字 段) < count( 主 键 id) < count( 1) ≈ count(*)

* Count（列）会计算列或这列的组合不为空的计数。
* 如果表只有一个字段的话那count(*)就是最快的
*  count(*) 跟 count(1) 的结果一样，都包括对NULL的统计，而count([列]) 是不包括NULL的统计
*  对于计数，也可以通过创建列为表名、total的表进行计数，利用事务能力，一般是先insert在update，理由是并发进行total值的更新时，是会上行锁的，如果先update total值可能会导致事务处理时间过长

## having的使用

* having一般需要搭配 group by 使用，在group by之后，order by之前

* having一般配合聚合函数使用，而where后面不能加聚合函数

* where是对**表的字段**进行条件过滤，having是对**select出来的字段**进行条件过滤

  可以想成 查询一些字段，先通过where进行一次过滤，group by进行一次分组，having对分组后的结果再过滤一次，having后的字段必须出现在select中

常见一点的sql，比如有如下表，这里为了方便理解以中文的形式表示字段

```sql
+-----+--------Log------+------------+
|  id | 网站名称 | 点击数 | date        |
+-----+---------+-------+------------+
|   1 |       A |    10 | 2016-05-10 |
|   2 |       C |    60 | 2016-05-13 |
|   3 |       A |   230 | 2016-05-14 |
|   4 |       B |    45 | 2016-05-14 |
|   5 |       E |   545 | 2016-05-14 |
|   6 |       D |    13 | 2016-05-15 |
|   7 |       C |   105 | 2016-05-15 |
|   8 |       E |   660 | 2016-05-16 |
|   9 |       C |   301 | 2016-05-17 |
+-----+---------+-------+------------+
```

查询 除了D网站外 各个网站的点击数 大于100 的 网站名称 和 点击数 并 降序 表示

select 网站名称, **SUM(点击数)**   
from Log where 网站名称!='D'   
group by 网站名称 **having** **SUM(点击数)**>100 order by SUM(点击数)  

## group by

* 全字段排序：查询条件是索引，但是order by 条件不是，会先遍历索引，再回表取值，每次取到数据就丢sort_buffer，完了之后在sort_buffer里根据order by条件排序 （利用sort_buffer + 临时表），会根据数据量采用内存排序或者外部归并排序，
* rowId排序：如果select的字段太多，超过设置的最大长度```max_length_for_sort_data```，就会只取主键和order by的条件丢进去sort_buffer里进行排序，最后再回表根据主键取出其他select的字段
* 如果order by的条件正好是索引顺序，就不需要使用sort_buffer进行排序了，直接使用索引顺序即可
* order by rand()，随机排序，使用内存临时表，使用rowId + 随机数进行排序
* 不带查询条件进行order by，就算order by条件是索引，是不一定会走索引进行排序，原因是如果MySQL优化器判断走索引后要去回表数量太大，就不会走
* 带limit的order by，mysql会采用堆排
* 默认的临时内存表是16M，由```tmp_table_size```设置

## 日期类查询

- curdate()函数：得到今天的日期，格式： 年-月-日

- now()函数：得到今天的日期和时间，格式：年-月-日 时:分:秒

- 两个datetime类型的字段相减，得到的单位跟日期的格式有关，如果格式有到秒，那减出来就是多少秒，如果格式只到日，那减出来就是多少日

- UNIX_TIMESTAMP(datetime类型的字段) 将datetime类型的字段转换为时间戳，要注意时间戳是以1970 年 1 月 1 日开始算的

- DATE_SUB(date, INTERVAL expr type) 函数：从日期减去指定的时间间隔

  date_format(date字段, ‘%Y%m%d %H:%i:%s') 函数：日期格式化函数，

  可以利用这些来查询最近多少天的数据如

  查询 近一小时的数据 where date字段 >= DATE_SUB(now(), INTERVAL 1 Hour) and date字段 < now()

  查询 昨天的数据 where date字段 >= DATE_SUB(CURDATE(), INTERVAL 1 Day) and date字段 < CURDATE()

  查询 近7天的数据 where date字段 >= DATE_SUB(CURDATE(), INTERVAL 7 Day) 

  查询 本月的数据 where date_format(date字段, ‘%Y%m') = date_format(curdate() , ‘%Y%m')

  查询 上个月的数据 where period_diff(date_format(now() , ‘%Y%m') , date_format(date字段, ‘%Y%m')) =1

  查询 今年的数据 where YEAR(date字段)=YEAR(NOW())

  查询 去年的数据  where YEAR(date字段)=year(date_sub(now(),interval 1 year))

  查询本季和上一季的跟 查年的差不多 ，把 YEAR函数 换成 QUARTER函数 即可

# 数据类型

## Datetime

* 保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间
* 与时区无关

## TimeStamp

* 和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年

* 它和时区有关，每个时间戳在不同时区所代表的具体时间不同

* 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间

* MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，

  提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳

# 索引

## 1.常见索引及概念

* 聚集索引：InnoDB中的主键索引，每张表的主键构造一棵B+树，同时叶子节点中存放的即为整张表的行记录数据。一个表只能包含一个聚集索引，聚集索引通常提供更快的数据访问速度。

* 非聚集索引：表中行的物理顺序与键值的逻辑顺序不匹配，查到记录对应的主键值 ，再使用主键的值通过聚集索引查找到需要的数据，这个过程也称为回表。

  要细分的话可以分为普通索引，唯一索引，组合索引，全文索引这些。

* 稠密索引：每个索引对应一个值

* 稀疏索引：每个索引对应一个存储块

* 覆盖索引：要查询的字段只需要去查询索引表就可以

* 组合索引：最左匹配，建立一个组合索引等于建立多个索引，能达到覆盖索引的目的，效率高；例如有组合索引(a，b，c)，则同时得到了索引(a)，(a，b)，（a，b，c），在 MySQL5.6 有个索引下推，才会在当查询条件带a，b的时候，会先查找索引树匹配a，再判断b，然后才回表找数据，从而减少回表次数

  组合索引之索引是最左匹配跟B+树有关，也是类似Order by的过滤，根据索引依次排列数据的，如Order by a,b,c 则先排a，a相同再排b，b相同再排c

索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间，**但主键索引的重建会导致整个表重建，一般可以使用alter table T engine=InnoDB来达到重建主键索引的效果**

## 2.特点

需要建立的列：经常需要搜索的列、主键列、外键列、排序的列、经常在where后面出现的列

* 避免进行数据库全表的扫描，大多数情况，只需要扫描较少的索引页和数据页，而不是查询所有数据页。而且对于非聚集索引，有时不需要访问数据页即可得到数据。
* 聚集索引可以避免数据插入操作，集中于表的最后一个数据页面。
* 在某些情况下，索引可以避免排序操作
* 加速表与表之间的连接
* 在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间
* 增，删，改会带来不小性能开销

## 3.原理

b+树：表的数据为叶子节点，非叶子节点为索引，有两条路径，一条是树，一条是各叶子相连	（mysql）

注意

![B+树](https://github.com/Nixum/Java-Note/blob/master/Note/picture/B+树.png)

注：

* N叉树的N在MySQL5.6后可以通过page大小来间接控制，叶子节点是page，一个page可以包含多个行
* B+树的插入可能会引起数据页的分裂，删除可能会引起数据页的合并，二者都是比较重的IO消耗，所以比较好的方式是顺序插入数据，这也是我们一般使用自增主键的原因之一



b-树，也称b树：所有节点为表的数据，只有一条路，从根节点开始

![B树](https://github.com/Nixum/Java-Note/blob/master/Note/picture/B树.png)                                                                                

### 为什么说B+树比B树更适合数据库索引

> B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。
>
> B+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。
>
> 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。

### 与二叉查找树、AVL树的比较

* AVL树的出度为2，而且AVL树要严格保持平衡，但旋转保持平衡比较耗时，适合用于插入删除次数比较少，但查找多的情况
* 二叉查找树在查找最大值或最小值的时候，二叉查找树就完全退化成了线性结构了
* 其他缺点同下面

### 与红黑树的比较

- 红黑树出度为2，B+树出度不止2，因此红黑树的高度会比B+树高，查找的次数也多了。（红黑树不是严格的平衡二叉树，旋转次数相对少，高度比平衡二叉树的低些）

- B+树在降低磁盘I/O操作数方面占优势

  > 为了减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，速度会非常快。
  >
  > 操作系统一般将内存和磁盘分割成固态大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。并且可以利用预读特性，相邻的节点也能够被预先载入。

参考[一步步分析为什么B+树适合作为索引的结构](https://www.cnblogs.com/aspirant/p/9214485.html)

## 4.失效情况

在使用到索引列的情况下

* 对索引列作运算如 + - * / !
* 索引属性出错，比如该索引列是字符串，但是写时候没有加``号，字符串和数字比较，**MySQL会把字符串转为数字，无法转成数字的字符串都会被转换成0**；或者是字段的字符编码格式不同，，比如有两张表，它们有外键关系，但是由于一张表是UTF-8另一张表是UTF-8mb4，也会导致索引失效；它们的原因都是MySQL在进行比较的时候，会使用函数对字段进行转换
* 模糊查询 like ‘%keyword%` 查询不能有前置的%，如果是 like ‘keyword%’ 这样还是可以用到索引的
* 索引列里有字段为null，null值不会加入到索引中
* 使用or连接条件，如果or连接的条件中有一个不是索引，会失效，可以改成使用union all来连接两条sql语句
* 组合索引没有体现最左匹配
* is null / is not null 对索引作判断
* 索引上使用  != 或者 <> 还有not in
* 索引的值只有几种情况，如性别只有男和女，这种情况虽然也会用索引，只是意义不大
* 表的量级较小，存储引擎判定使用全表扫描更快
* 有一case：1.select * from T where k in(1,2,3,4,5)；2.select * from T where k between 1 and 5，k为索引，但是推荐使用方法2，因为方法1会导致树查5次，而2是1次
* 对索引字段使用了函数进行计算，可能会导致MySQL不使用该索引，或者进行了全索引扫描，无法用到索引进行快速定位
* 如果查询的值的长度是否大于索引定义的长度，如果大于，**虽然也会走索引**，因为MySQL是先把查询值的长度截断成跟索引定义的长度一致去遍历索引，但是它还要再回表得到数据进行比较，所以查询会很慢

## 5.优化

* 注意区分度，计算索引最优长度，使用这个计算 ```count(distinct  left(列名,  索引长度)) / count(*) from table```，**区分度越高越好**，另外使用前缀索引虽然会减少索引存储空间，但是可能会增加扫描次数或者覆盖索引不生效

* 当表的字符集编码或者属性不同时，需要想办法把函数加再索引对应的值上，而不是索引字段上，或者将去掉函数，使用其他方法替代

* 当要充当索引的字段在某些长度的区分度太小时，可以增加一个字段，采用索引字段的倒序存储或者hash的方法来充当索引，缺点是无法使用索引进行范围查询，而hash更是只能支持等值查询，查询时需要进行额外的计算，也是一种性能消耗

* 由于MySQL在选择索引的时候会根据 索引区分度 和索引对应的预估扫描行数（包括回表），但是有可能预估的结果是不准的，如果通过explain命令发现rows的值与想象中的偏差较大，可以执行```analyze table [tableName]```来重新统计索引信息，或者使用```force index([索引名称])```来强制使用索引，或者重写SQL，引导MySQL使用正确的索引

* 注意索引的最左前缀原则，如果在设置联合索引时，可以通过调整顺序来达到少维护一个索引，拿这个顺序就可以优先考虑，因为一个要考虑的就是索引的大小

* 关于普通索引和唯一索引

  在查询上，唯一索引和普通索引的区别是，唯一索引在查找到结果后就不会继续往下查了，但其实性能跟普通索引差别不会很大，但是更新的时候唯一索引由于用不上change buffer机制，更新的性能比较差

  在更新上，InnoDB会先判断更新的数据是否在内存，如果在就直接更新内存，如果不在，就把更新操作写到change buffer，等到数据加载到内存，在从change buffer里将更新操作更新到内容。change buff只适用普通索引上的更新操作，因为唯一索引因为需要先读取所有数据，判断索引是否重复后再插入，如果此时数据没有被读进内存，需要磁盘随机IO读取，最终导致更新变慢。

  另外，为了保证更新操作的稳定性，实际上在写内存的过程中还会把相关操作记录按顺序写进redo log（磁盘），才算真正完成更新操作。查询的时候其实可以直接查内存里的数据（内存已更新），或者先把磁盘里的数据读到内存，再配合change buffer就能得到更新后的数据了

## 6.分析

### 使用explain

Explain + SQL语句，给出该SQL语句的分析结果，看看查询的类型，有没有用到索引，是不是全表扫描

比较重要的字段：

* **select_type**：查询类型，如 简单查询、联合查询、子查询等
* **type**：访问类型，ALL(全表扫描)、index(索引查询)、range(索引范围查询)、ref(表间的连接匹配条件)、const(常量)，一个好的SQL起码得达到range级
* **Key**：索引列的名称
* **rows**：扫描的行数
* **extra**：额外信息说明，using index指使用到了覆盖索引，using index condition指使用了索引下推

### 使用show processlist

此命令用于查看目前执行的sql语句执行的状态

### 使用performance_chema和sys系统库

* MySQL启动前需要设置```performance_schema=on```，但是性能会比off少10%
* 查询sys.schema_table_lock_waits、sys.innodb_lock_waits表可以知道那条语句在占用锁
* 查看 information_schema.innodb_trx表可以看到事务具体的状态

### 慢查询分析

慢查询日志中，rows_examined字段，表示某个语句执行过程中扫描了多少行

# 存储引擎

## 1.MyISAM

设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。

提供了大量的特性，包括压缩表、空间数据索引等。

不支持事务。

不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。

可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。

如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。

## 2.InnoDB

是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。

实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻读。

主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。

内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。

支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。

关于有时更新或者查询时突然变慢的原因：首先，InnoDB更新采用WAL机制，即更新的时通过redo log记录更新操作，修改内存里的数据，然后再在恰当时间flush到磁盘，原因就是出在flush磁盘上，当redo log满了，或者内存满了，脏页太多，都会将内存里的数据flush到磁盘，以腾出空间，如果要刷的数据特别多，那消耗的时间就长。解决方法：1. 设置到 innodb_io_capacity参数，该参数会告诉InnoDB机器的磁盘能力，可以使用fio工具测出 2. 控制脏页的比例，设置innodb_max_dirty_pages_pct的值，默认是75%，达到了就会刷 3. 刷新脏页时是否会递归检测隔壁数据页是否也是脏页，如果是会连着一起刷，通过innodb_flush_neighbors=1表示采用这种机制，=0表示只会刷自己，这个机制对机械硬盘关系比较大，SSD则不会

**关于InnoDB的删除**：

```innodb_file_per_table=OFF```：表示表数据放在系统共享表空间，=on表示各个表空间放在独立文件下，后缀名是 .ibd ，一般设置=on，便于管理。

当我们使用delete删除数据时，InnoDB实际上是把数据页上的该数据标记为删除，表示该位置可以进行复用，此时磁盘上的文件并不会变小，当数据随机插入时会因为页分裂，分裂后的页可能存不满数据，就会标记某些位可复用，导致页的利用率不高，当有大量的增删时，会导致数据页存在大量空洞，为了压缩空间，此时的解决办法是重建表，一般使用```alter table [tableName] enging=InnoDB```达到重建的目的，MySQL会自动创建临时表，进行数据转存，交换表名，删除旧表，此时会阻塞，阻止增删改，**5.56版本后使用onlineDDL机制**，解决了这个问题，解决方法是使用redo log记录新插入的数据 + MDL读锁(写锁会退化) + IO + CPU

## 3.区别

* MyISAM是非事务安全；InnoDB是事务安全型
* MyISAM的锁是表锁；InnoDB支持行锁
* MyISAM支持全文索引；InnoDB不支持（5.6版本后才支持）
* MyISAM适合 `SELECT` 密集型的表；InnoDB适合 `INSERT` 和 `UPDATE` 密集型的表
* MyISAM表是保存成文件形式，跨平台转移方便
* InnoDB表比MyISAM表安全
* MyISAM对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用
* MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢；InnoDB支持安全恢复
* MyISAM不支持外键；InnoDB 支持外键，InnoDB 支持在线热备份

# 事务

## 1.ACID原则

原子性（atomicity）、一致性（consistency）、隔离性（isolation）和持久性（durability）

* 原子性：事务中的所有操作要么全部提交成功，要么全部失败回滚
* 一致性：数据库总是从一个一致性的状态转换到另一个一致性的状态
* 隔离性：一个事务所做的修改在最终提交以前，对其他事务是不可见的
* 持久性：一旦事务提交，则其所做的修改将永久保存到数据库

## 2.并发情况下带来的问题

* 脏读：如有事务A和B，A读取了B未提交的数据
* 不可重复读：如有事务A和B，A负责读取，B负责写入，A连续读的过程中B写入了一次，A前后两次读出来的数据不一样
* 丢失更新：如有事务A和B，AB均写入数据，A写入的数据被B覆盖
* 幻读：如有事务A和B，A修改表内数据的过程中，B向表内插入了一条数据，A修改完后发现数据并没有被全部修改完，或者事务A内前后两条相同的SQL带“当前读”查询查回来的数据数不一致

## 3.事务隔离级别

隔离级别就是为了解决上述并发时候带来的问题

* DEFAULT：默认隔离级别，即使用底层数据库默认的隔离级别；

* READ_UNCOMMITTED：未提交读，保证了读取过程中不会读取到非法数据。隔离级别在于处理多事务的并发问题

  可能出现 脏读、不可重复读、丢失更新、幻读；

* READ_COMMITTED：提交读，一个事务提交之后，它做的变更才会被其他事务看到，保证了一个事务不会 读 到另一个并行事务已修改但未提交的数据

  避免了“脏读”，可能出现不可重复读、丢失更新；

  Oracle默认隔离级别

* REPEATABLE_READ：可重复读，一个事务在执行中看到的数据，总是跟这个事务在启动时看到的数据一致，保证了一个事务不会 修改 已经由另一个事务读取但未提交（回滚）的数据。

  避免了脏读和不可重复读取，"丢失更新"的情况，可能存在幻读；

  mysql默认是此隔离级别；MySQL使用MVCC和间隙锁来防止 幻读

* SERIALIZABLE：序列化,最严格的级别，事务串行执行,即一个事务要等待另一个事务完成才可进行

  效率最差

  例子：

  | 事务A                 | 事务B       |
  | --------------------- | ----------- |
  | 启动事务，查询得到值1 | 启动事务    |
  |                       | 查询得到值1 |
  |                       | 将1改为2    |
  | 查询得到的值v1        |             |
  |                       | 提交事务B   |
  | 查询得到的值v2        |             |
  | 提交事务A             |             |
  | 查询得到值v3          |             |

  在不同隔离级别下的答案

  未提交读：v1=2，v2=2，v3=2

  提交读：v1=1，v2=2，v3=2

  可重复读：v1=1，v2=1，v3=2

  串行：v1=1，v2=1，v3=2，且直到事务A提交后，事务B才可以继续执行

一般避免使用长事务，即在一个事务里做过多操作，长事务会导致回滚日志变大，也会占用锁资源

## 4.事务相关命令

1. 显示启动事务，使用begin或strart transaction启动事务，commit提交事务，rollback回滚
2. set autocommit=0，关掉自动提交，只要执行了任意语句，事务就开启了，并且不会自动提交，直到显示使用commit、rollback或断开连接，一般是使用set autocommit=1，开启事务，再commit提交事务，执行commit work and chain则提交事务并开启下一次事务

## 4.传播行为（Spring）

* PROPERGATION_MANDATORY：方法必须运行在事务中，如果当前事务不存在，抛异常
* PROPAGATION_NESTED：当前事务存在，则该方法运行在嵌套事务中
* PROPAGATION_NEVER：方法不能运行事务中，否则抛异常
* PROPAGATION_REQUIRED：当前方法必须运行在事务中，如果当前存在事务，该方法运行其中，否则创建新事务
* PROPAGATION_REQUIRES_NEW：当前方法必须运行在事务中，如果当前存在事务，则该事务在方法运行期间被挂起
* PROPAGATION_SUPPORTS：当前方法不需要运行在事务中，但如果存在事务，也可运行在事务中
* PROPAGATION_NOT_SUPPORTED：当前方法不能运行在事务中，如果存在事务，则挂起该方法

# 锁

根据范围，可以分为全局锁、表级锁、行锁，当多种锁同时出现时，必须得所有锁不互斥，才能并行，否则就得等

* **全局锁**：对整个数据库实例加锁，命令：``` Flush tables with read lock```，让整个数据库变成只读，禁止任何ddl、dml语句

  一般用于全库逻辑备份，但有可能造成主从库数据延迟或者业务停摆，不用的话又会导致数据不一致问题，一般这种方式是给不支持 可重复读 事务的引擎使用的，像InnoDB可以在可重复读隔离级别下开启事务读数据，利用MVCC来保证在此期间数据一致

* **表级锁**：

  一种需要显示启动，比如```lock tables t1 read, t2 write;``` 表示线程在执行unlock tables之前，只能读t1，读写t2，其他操作做不了。

  另一种是 MDL(metadata lock)，不需要显示使用，在访问一个表时自动加上，作用是保证读写正确性，当对一个表内数据做CRUD时，加MDL读锁，当对一个表做结构变更时，加MDL写锁。

  MDL锁和表锁时可以同时出现的，比如MyISAM表上更新一行，会加上MDL读锁和表锁

  读锁间不互斥，读写、写写间互斥，MDL会在事务提交后释放。当需要对热点表做结构变更时，最好在变更语句上加等待时间，避免出现死锁导致整个表无法读写，

* **行锁**：在InnoDB事务中，采用两阶段锁协议，行锁是在事务结束后才释放，在事务过程中，即使一开始用了后面没用到也不会被释放，因此，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放，减少锁的影响时间。

* **多版本并发控制（MVCC）**：用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 这样在读操作不用阻塞写操作，写操作不用阻塞读操作的同时，避免了脏读和不可重复读。MVCC 在语境中倾向于 “对多行数据打快照造平行宇宙”，然而 CAS 一般只是保护单行数据而已

  例子：

  假如一个值1被按顺序改为2、3、4，每一次更改都会记录在回滚日志里，如：将2改为1 -> 将3改为2 -> 将4改为3，当前的值是4，在查询这条记录的时候，不同时刻启动的事务会有不同的视图，比如有视图A（将2改成1）里看到的值是1，即同一条记录在数据库中存在多个版本

  MVCC的版本快照，指的是什么呢，而且它是基于整个库的，总不能保存多个版本的库的所有数据吧？实际上MVCC得到的快照是逻辑上的数据，是推测出来的，通过当前值，利用事务Id和undo log日志，根据日志"回滚"得到各个版本的数据，事务id可以简单理解为对该行数据进行更改时产生的id，当然一个事务内可以对多条数据进行操作，这多条数据的事务id都是相同的

* **共享锁**（SQL + lock in share mode）：行锁，事务T对数据A加上共享锁，其他事务只能对A添加共享锁，不能加排他锁，获取共享锁的事务只能读不能写

* **排他锁**（SQL + for update）：行锁，事务T对数据A加上排他锁，则其他事务不能再对A加任何类型的锁。获得排它锁的事务即能读数据又能修改数据

* **间隙锁**：当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做间隙锁，理论上会在范围查询前后第一个不满足条件的值，这个范围上加左开右闭范围的间隙锁。

  主要是为了解决幻读问题，只在可重复读隔离级别下有效，一般与行锁一同出现。间隙锁也会导致死锁，比如两个事务同时在一段范围内的数据加入间隙锁，又insert或update在这个范围内的数据，就会导致死锁

可重复读隔离级别时的加锁规则：

1. 查询过程中扫描到的行才会加锁，锁的基本单位是next-key lock（左开右闭）
2. 如果查询的对象不存在，会在该查询条件所在的位置的前后遇到的第一个存在的数据的这段范围加上间隙锁
3. 索引上的等值查询，如果是唯一索引，加的是行锁，如果非唯一索引，需要访问到第一个不满足条件的值，这个范围加上间隙锁（左开右开）
4. 范围查询上，无论是否是唯一索引，范围查询都需要访问到第一个不满足条件的值为值，这个范围加间隙锁（左开右闭）

以上规则需要组合起来使用，即，如果查询id=8，是有可能锁的范围是(5,8]，唯一索引叶子节点id=8的上层节点可能是5，

InnoDB会对扫描过的行都加上行锁和间隙锁，所以如果查询条件不是索引，此时锁的是整张表，因此也可以理解锁是加在索引上的

**关于死锁**

当线程出现循环资源依赖，导致多个线程互相等待的状态称为死锁，解决方案：

* 设置超时时间 ```innodb_lock_wait_timeout```，在InnoDB中默认是50s
* 死锁检测，当发现死锁后，主动回滚死锁链条中的某一个事务，设置```innodb_deadlock_detect=on```，默认值是on

一般使用第二种，但死锁检测会消耗大量CPU资源，主要发生在对同一行进行更新的检测上，其算法是O(n)，虽然在同一行更新不会造成死锁，但是当并发很高时进行检测时就会消耗大量CPU资源，解决方案有两种，一种是在中间件或者MySQL server层上，增加对同一行更新的判断，进行排队，或者将那一行改为逻辑上的多行，来分散压力

**关于事务可见性的疑问**

在可重复读的隔离级别下，事务A开启，操作数据D，此时会创建一个当前数据D的视图(MVCC)，但此时又刚好有事务B，已经操作到这条数据，并给这条数据加了行锁，对这条数据进行操作，事务B操作完成后，释放数据D的行锁，那之前的事务A在最终修改操作数据D时，数据D的值是什么呢？

假如一开始k的值是1，autocommit=1，即单独一条SQL执行本身就是一个事务，会自动提交

**另外，事务中的begin / start transaction命令，是以执行它们之后的第一个sql语句为启动开始事务，而start with consisten snapshot是以这条命令为起点开启事务**

| 事务A                                      | 事务B                                      | 事务C                               |
| ------------------------------------------ | ------------------------------------------ | ----------------------------------- |
| start transaction with consisten snapshot; |                                            |                                     |
|                                            | start transaction with consisten snapshot; |                                     |
|                                            |                                            | update t set k = k + 1 where id = D |
|                                            | update t set k = k + 1 where id = D;       |                                     |
|                                            | select K from t where id = D;              |                                     |
| select k from t where id = D;              |                                            |                                     |
| commit;                                    |                                            |                                     |
|                                            | commit;                                    |                                     |

上面这道题的答案是 事务A查到k的值分别是1，事务B查到k的值是3

原因：事务A之所以查到的值是1，是因为事务A开启时，事务B和C还没开启，此时的快照k=1，因此得到的值是1。事务B查到k的值是3，虽然事务C是在事务B之后开启的，感觉看不到事务C修改后的值，但是由于更新操作是先读后写的，此时的读是**当前读**（当前读总是读该已提交的数据的最新版本），而当前读的值是2，因此更新后k的值是3，如果不这么做，就会导致事务C更新丢失，而在同一个事务内读值，是可以读到由当前事务修改的值的，所以事务B读到的值是3。普通select语句，在可重复读情况下，为了实现一致性读，是通过读undo log实现的，如果undo log太长（可能因为更新次数太多），会导致查的很慢

题外话，如果事务A读的时候加锁，就会变成当前读，例如将事务A的select语句后面加上lock in share mode（共享锁）或者for update（排他锁），那么查到的值就是3了。如果事务B在执行更新前先select了，查到的值也是1。如果事务C是显式启动事务，在事务B select后commit前才执行commit操作，就会触发二阶段锁协议，两条更新语句同时更新一行数据，先执行的语句会对这条数据加行锁，所以事务B需要等到事务C提交后，才能执行更新操作

这个问题的关键在于要理解 MVCC原理，更新操作前的当前读，事务的隔离级别，一致性读、行锁

参考：

[Mysql加锁过程详解（1）-基本知识](https://www.cnblogs.com/crazylqy/p/7611069.html)

[Mysql中的锁机制](https://www.cnblogs.com/leedaily/p/8378779.html)

[Innodb中的事务隔离级别和锁的关系](https://tech.meituan.com/2014/08/20/innodb-lock.html)

# CAP理论

[CAP 定理的含义](http://www.ruanyifeng.com/blog/2018/07/cap.html)，CAP是针对分布式数据库的

还有BASE，可以理解成低配版CAP，达到一个基本的标准

# 大表优化

[MySQL大表优化方案](https://segmentfault.com/a/1190000006158186)

# 分库分表

## 分表

* 分为水平拆分(对行)和垂直拆分(对列)

水平拆分，针对某一列做哈希取模，平均分配到各个分表中，如果要扩容，由于模会变，导致数据要重新哈希，停机迁移数据，这样是不行的，因此一开始要设计好，例如使用一致性哈希算法，减少迁移的数据量；或者分表的时候取2的n次方，这样扩容的时候也以2的n次进行扩容，这样原来的key重新取模是在原来的位置或者原来的2倍；或者换个数据库了，像上面大表优化里面提到的那样，其他的目前还想不出还有什么方案

# 数据部分

## 数据存储和恢复

只要redo log 和 bin log能够持久化到磁盘，就能确保MySQL异常重启后，数据就可以恢复

### 原理

* bin log的写入机制：事务执行过程中，先把日志写进bin log cache，事务提交时，再把bin log cache写进bin log文件（先写到文件系统的page cache，再进行持久化）中，然后把bin log cache清空。

  bin log cache每个线程自己维护，bin log的写入是一个顺序操作

  bin log cache的大小通过```binlog_cache_size```控制，如果超过就暂存到磁盘

  ```sync_binlog=x```表示每次事务提交都会write到磁盘page cahce，但是会累计提交x个事务后才把bin log的数据持久化到磁盘，一般设置范围是100~1000，对应的风险是，如果机器宕机，会丢失最近N个事务的bin log日志

* redo log写入机制：原理与bin log类似，但是它是二阶段提交，有状态，事务执行过程中，redo log先prepare状态，写入redo log buffer，再写bin log，提交事务，变为redo log commit状态

  redo log buffer全局共用，与bin log cache不同

  ```innodb_flush_log_at_trx_commit```=0，表示每次事务提交只是把redo log写入redo log buffer，=1 表示每次事务提交会持久化redo log到硬盘中，=2 表示每次事务提交会把redo log写到page cache，innodb后台有进程每秒钟将redo log buffer中的日志写进page cache，再持久化到磁盘，所以有可能会把事务未提交的redo log持久化到硬盘

一般会把```sync_binlog```和```innodb_flush_log_at_trx_commit```都设置为1，即一个事务完整提交前，会刷两次盘。另一种设置是让```sync_binlog=1000```和```innodb_flush_log_at_trx_commit=2```，一般是在主备复制存在很大延迟时，为了让从库的备份速度跟上主库

为了提高刷盘效率，MySQL一般会让多个事务在一段时间内完成，或尽量让page cache里的redo log和bin log组合在一起提交，减少刷盘次数

## 主从复制

一般从库设置为read only，可以避免主从切换过程的双写，实现的是最终一致性。

### 原理

利用MySQL中的bin-log二进制文件，该文件记录了所有sql信息，把主数据库的bin-log文件发送给从数据库，在从数据库的relay-log重做日志文件中利用这些信息进行恢复。

bin log分为三种格式

* statement：记录每次执行的SQL，但由于索引选择问题或者SQL语句使用聚合函数，有可能会导致主从不一致的问题
* row：记录的是事件，表示每条SQL语句执行后的数据信息，比如delete操作后，会记录delete事件和delete删除的行的所有字段(可设置为记录所有字段或者只记录主键)；update操作会记录行数据前后的记录；insert操作会记录insert的所有字段信息，缺点是占空间，但对数据的恢复有利
* mix（混合上面两种）：由MySQL自己判断，如果会出现主从不一致，就使用row，否则使用statement

bin log上会记录每台机子的server id，用于避免循环复制

### 具体步骤

每个MySQL数据库上都有这三个线程

- **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（bin log）中。
- **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的重放日志（replay log）中。
- **SQL 线程** ：负责读取replay log日志并重放其中的 SQL 语句。

![MySQL主从复制](https://github.com/Nixum/Java-Note/blob/master/Note/picture/MySQL主从复制.png)

针对复制同步带来的读延时问题，可以采取 一主多从，主写从读，分散压力；利用好缓存中间件；持久化层的处理

### 主备延迟

当备库重放日志的速度小于主库产生bin log的速度，会出现主备延迟，可能的原因：

* 主备机器配置不一致，备库机器性能较差
* 备库压力大，比如在备库上进行SQL分析、大量查询、大表的DDL，主库上的长事务操作等，消耗大量CPU资源导致

### 主备切换策略

* 可靠性优先：前提，备库是只读的，首先，备库持续判断与主库同步间的延迟时间，如果小于可接受的值，主库改为只读，主库等待备库同步延迟时间降为0，备库改为可读写，业务切换到备库，业务不可写的时间取决于主库等待备库同步数据的延迟时间
* 可用性优先：步骤与上面的类似，只是主库改为只读后，不等待备库同步完数据，就切到备库，此时数据会不一致，后面再自己根据bin log手动调整更正

MySQL的高可用（通过主库发生故障时切到从库），是依赖主备复制的，主备延迟时间越小，可用性越高

## 读写分离

主数据库负责写，从数据库负责读，从而缓解锁的争用、节约系统开销

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器

[MySQL主从复制](https://www.cnblogs.com/a8457013/p/7819018.html)

# JDBC

SUN的 JDBC 是一套接口，而实现是各个数据库厂商的驱动包，因此使用了桥接模式

DriverManager注册驱动包，com.mysql.jdbc.Driver类中的static块会创建驱动实例，因此只需要把驱动字节码加载到JVM里即可，Class.forName(“com.mysql.jdbc.Driver”); 

Connection conn = DriverManager.getConnection(url, username, password)获取连接

Statement stmt = con.createStatement(); 之后使用stmt的方法执行SQL语句即可，返回ResultSet

ResultSet下标从1开始

使用Connection类的setAutoCommit(false) 方法来实现事务，以这个开始，Connection类的commit()方法提交，Connection类的rollback()方法回滚

最后关闭ResultSet、Statement和Connection

# 数据库连接池

链表实现，在使用连接对象之前，先创建好一定数量的连接对象，以链表的形式连接，从端首取，用完回到段尾。

当池子中没有连接对象可取时，就让其先等待，如果等待超时还没有回获取到连接对象，就新建一个连接对象让其使用，用完后销毁该创建的对象

连接池负责管理、监控和维护这些连接对象

连接池单例

连接池需要保证线程安全

# 参考

[MySQL ACID及四种隔离级别的解释](https://www.cnblogs.com/xuanzhi201111/p/4103696.html)

[CyC2018/CS-Notes/MySQL](https://github.com/CyC2018/CS-Notes/blob/master/docs/notes/MySQL.md)

[极客时间 - MySQL实战45讲]()

后记

极客时间 - MySQL实战45讲真的是质量很高的讲MySQL的课程，解决了我很多事务方面的疑惑